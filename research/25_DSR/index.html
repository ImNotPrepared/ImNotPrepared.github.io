<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"

        content="Why is Sparse-View 4D Reconstruction Hard?">
  <meta name="keywords" content="4D Dynamics Scene Reconstruction, Free-view Synthesis, Sparse View">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>4D Recon</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->
  <link rel="icon" type="image/png" href="./static/images/ri.png"> 
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/ri.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://imnotprepared.github.io/research/25_DSR">
            4D Scene Recon - in submission
          </a>
          <a class="navbar-item" href="https://imnotprepared.github.io/research/IROS24_AirShot">
            AirShot - IROS 2024
          </a>
          <a class="navbar-item" href="https://imnotprepared.github.io/research/ICLR24_ONLS/">
            ONLS - ICLR 2024
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="./static/images/tteaser.svg" width="300">
          <h1 class="title is-2 publication-title">Why is Sparse-View 4D Reconstruction Hard?</h1>
          <div class="column is-full_width">
            <h2 class="title is-3">in submission</h2>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://imnotprepared.github.io/">Zihan Wang</a>,</span>
            <span class="author-block">
              <a href="https://jefftan969.github.io/">Jeff Tan</a>,</span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~tkhurana/">Tarasha Khurana</a>,
            </span>
            <span class="author-block">
              <a href="https://www.neeharperi.com/">Neehar Peri</a>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan*</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Carnegie Mellon University
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/10XsArUhlpraBpuiVFpCH1TOwCm5C-dgy/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://drive.google.com/file/d/10XsArUhlpraBpuiVFpCH1TOwCm5C-dgy/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv(soon)</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code(soon)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data (ExoRecon, soon)</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div style="text-align:center;">
        <img src="static\images\teaser.png" style="width:100%; height:auto;">
      </div>
      <!-- <img class="rounded" src="./media/nice-slam/teaser.png" > -->
      <br>
      <h2 class="subtitle has-text-centered">
        Given <b>sparse-view videos of dynamic scenes</b>, our approach reconstructs 3D geometry and motion, enabling extreme novel view synthesis, 3D tracking, and feature distillation. <b>Our sparse-view (4-camera) setup strikes a balance</b> between ill-posed reconstructions from casual monocular captures and well-constrained reconstructions from dense multi-view studio captures.
      </h2>
    </div>

    <h2 class="subtitle has-text-centered">
      We show results in diverse scene such as bike repair, healthcare(CPR) and dancing! 
    </h2>
    <h1> 
      Complex, High occluded scene: Bike Repair
    </h1>
    <div class="hero-body">
      <div style="text-align:center;">
        <video id="teaser1" autoplay muted loop style="flex: 50%; max-width: 100%;">
          <source src="./static/images/bike.mov" type="video/mp4">
      </video>
      </div>

      <div style="text-align:center;">
        <video id="teaser1" autoplay muted loop style="flex: 50%; max-width: 100%;">
          <source src="./static/images/csr.mov" type="video/mp4">
      </video>
      </div>

      <div style="text-align:center;">
        <video id="teaser1" autoplay muted loop style="flex: 50%; max-width: 100%;">
          <source src="./static/images/dance.mov" type="video/mp4">
      </video>
      </div>
      <!-- <img class="rounded" src="./media/nice-slam/teaser.png" > -->
      <br>
    </div>
    <!--         <iframe width="800" height="400" src="https://www.youtube.com/embed/ycivCw1LmdE?si=zstMsf33Lgjytup5" title="Intro to PVT++" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
 -->
</section>

<section class="section">
  <div class="container is-max-desktop">
        <!-- Paper video. -->
        <!-- <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Video</h2>
            <div class="publication-video">
              <iframe src="https://www.youtube.com/embed/V5hYTz5os0M?rel=0&amp;showinfo=0"
                      frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </div>
          </div>
        </div> -->
        <!--/ Paper video. -->
      
    <!-- <br> -->
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We address the problem of <b>dynamic scene reconstruction from sparse-view videos.</b> Prior work often requires <b>dense multi-view captures using dozens of calibrated cameras</b> (e.g. Panoptic Studio), or <b>short monocular videos with limited information</b> (e.g. DAVIS). In contrast, we aim to reconstruct diverse dynamic human behaviors, such as repairing a bike or dancing from sparse-view videos. 
          </p>
          <p>
            We repurpose state-of-the-art monocular reconstruction methods for <b>sparse-view reconstruction</b> and find that <b>careful initialization from time- and view-consistent monocular depth estimators</b> produces more accurate reconstructions. 
            Specifically, our method predicts dense surface points across all training views, and uses confidence-aware pixel alignment to initialize scene geometry. 
            We further distill per-point semantic features from 2D foundation models, and use <b> feature clustering</b> to encode a compact set of motion bases. 
            Finally, we employ a gradient-based joint optimization framework to simultaneously learn scene geometry and motion. 
          </p>
          <p>
            Notably, our approach achieves <b>state-of-the-art performance</b> on challenging sequences from the Ego-Exo4D dataset.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- <br> -->
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Contribution</h2>
        <div class="content has-text-justified">
          <li>
            We repurpose Ego-Exo4D for sparse-view reconstruction and highlight the challenge of reconstructing skilled human behaviors in dynamic environments.
          <li>
            We demonstrate that monocular reconstruction methods can be extended to the sparse-view setting by carefully incorporating monocular depth and foundational priors.
          </li>
          <li>
            We extensively ablate our design choices and show that we achieve state-of-the-art performance on challenging sequences from Ego-Exo4D.
          </li>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method Section -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <hr>
        <h2 class="title is-3">Method Overview</h2>
        <br>
        <div class="content has-text-justified">
          <img src="static/images/method.png" alt="Structure" class="center" />
          <p>As dynamic scene reconstruction from sparse views is extremely challenging, we present two key insights to initialize plausible geometry and motion:</p>
              <li>Initializing consistent scene geometry via confidence-aware spatio-temporal alignment</li>
              <li>Initializing motion trajectories by clustering per-point 3D semantic features distilled from 2D foundation models</li>
            </ul>
          </ul>
        </div>
    

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method Section -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <hr>
        <h2 class="title is-3">Experiments</h2>
        <br>
        <div class="content has-text-justified">
          <h4>Concrete steps to validate our points:</h4>
          <ul>
              <li>
                  <strong>Perfect training views</strong> 
                  <span>(prove correct implementation)</span>
              </li>
              <li>
                  <strong>Perfect near-novel (5&deg;) view synthesis</strong> 
                  <span>(illustrate that the method works, like other approaches)</span>
              </li>
              <li>
                  <strong>Great extreme-novel (45&deg;) view synthesis</strong> 
                  <span>(demonstrate free-viewpoint rendering, no one did it before)</span>
              </li>
              <li>
                  <strong>Held-out camera evaluation (90&deg;)</strong> 
                  <span>(use 3 out of 4 cameras for training and leave 1 out for qualitative results)</span>
              </li>
          </ul>

          <div class="columns is-centered has-text-centered">
            <div class="column is-full-width">
              <h3>Selected Qualitative Results</h3>

          
              <div class="content has-text-justified">
                <h5>Novel View Synthesis Comparison</h5>
                <img src="static/images/result0.jpg" alt="Structure" class="center" />
                <p>
                  In each <strong>2×3</strong> block, we visualize the rasterized RGB image, depth map, and foreground mask from each
                  method for the dancing (top) and bike repair (bottom) sequences. We show <strong>45°</strong> (left) and <strong>90°</strong> (right) 
                  novel view synthesis results.
              </p>
              <p>
                  Existing monocular methods and their extension to multi-view produce poor results rendered from a drastically different novel view.
                  <strong>MV-SOM</strong> improves upon SOM in both <strong>45°</strong> novel-view synthesis and <strong>90°</strong> novel-view synthesis. 
                  Our method’s careful point cloud initialization and feature-based motion bases further improve on MV-SOM. 
              </p>
              <p>
                  Note that <strong>Dynamic 3DGS</strong> does not separate foreground and background, so we report a black foreground mask.
              </p>
                  </ul>
                </ul>


                <h5>Novel View Synthesis Results of Our Method</h5>
                <div class="content has-text-justified">
                  <img src="static/images/result1.jpg" alt="Structure" class="center" />
                    <p>
                        Results from more video sequences. In each row, we visualize the rasterized RGB image, depth map,
                        and foreground mask from our method for various diverse scenes, including music (top), cooking (middle), and healthcare (bottom).
                    </p>
                    <p>
                        We include results for <strong>5°</strong> (left) and <strong>45°</strong> (right) novel view synthesis results. Notably, the rendered 
                        RGB and depth maps produce consistent reconstructions and plausible geometry.
                    </p>
                <h5>More Results and Ablation Study</h5>
                <p>
                  For more details, please check out <a href="https://drive.google.com/file/d/10XsArUhlpraBpuiVFpCH1TOwCm5C-dgy/view" target="_blank">our paper</a>.
              </p>
                
                </div>
                


        </div>

<section class="section" >
  <div class="container is-max-desktop content">
    <h1 class="title">BibTeX</h1>
    yet to come, I will send you email to ask for citation (just kidding). 
  </div>
</section>


<section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h1 class="title">Acknowledgements</h1>
    We thank ourselves for surviving. We also extend our gratitude to Zihan for his generous payment of tuition fees to Carnegie Mellon University, which made this possible.
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            This webpage template is from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. 
            We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
          </p>
        </div>
      </div>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>