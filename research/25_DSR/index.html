<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"

        content="Why is Sparse-View 4D Reconstruction Hard?">
  <meta name="keywords" content="4D Dynamics Scene Reconstruction, Free-view Synthesis, Sparse View">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>4D Recon</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->
  <link rel="icon" type="image/png" href="./static/images/ri.png"> 
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/ri.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://imnotprepared.github.io/#Publications">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://imnotprepared.github.io/research/25_DSR">
            4D Scene Recon - in submission
          </a>
          <a class="navbar-item" href="https://imnotprepared.github.io/research/IROS24_AirShot">
            AirShot - IROS 2024
          </a>
          <a class="navbar-item" href="https://imnotprepared.github.io/research/ICLR24_ONLS/">
            ONLS - ICLR 2024
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="./static/images/tteaser.svg" width="300">
          <h1 class="title is-2 publication-title">MonoFusion: Sparse-View 4D Reconstruction via Monocular Fusion</h1>
          <div class="column is-full_width">
            <h2 class="title is-3">ICCV 2025</h2>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://imnotprepared.github.io/">Zihan Wang</a>,</span>
            <span class="author-block">
              <a href="https://jefftan969.github.io/">Jeff Tan</a>,</span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~tkhurana/">Tarasha Khurana</a>,
            </span>
            <span class="author-block">
              <a href="https://www.neeharperi.com/">Neehar Peri</a>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan*</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Carnegie Mellon University
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2507.23782"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>              
              <!-- Video Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/ImNotPrepared/MonoFusion"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code(soon)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/drive/folders/18H8OOOZLv7OmOen8pGbSLWwu8AvZAZro?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data (ExoRecon)</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div style="text-align:center;">
        <img src="static\images\teaser.png" style="width:100%; height:auto;">
      </div>
      <!-- <img class="rounded" src="./media/nice-slam/teaser.png" > -->
      <br>
      <h2 class="subtitle has-text-centered">
        Given <b>sparse-view videos of dynamic scenes</b>, our approach reconstructs 3D geometry and motion, enabling extreme novel view synthesis, 3D tracking, and feature distillation. <b>Our sparse-view (4-camera) setup strikes a balance</b> between ill-posed reconstructions from casual monocular captures and well-constrained reconstructions from dense multi-view studio captures.
      </h2>
    </div>

    <section class="section">
      <div class="container is-max-desktop">
        <!-- Method Section -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <hr> 
            <h2 class="title is-3"> 4D Scene Reconstruction that Supports Free-View Synthesis</h2>
            <br>
    <h2 class="subtitle has-text-centered">
      We show comprehensive results that cover all categories of egoexo4D! Even those complex, highly-occuluded scene. (We omit the training view for better visualization experience later)
    </h2>
    <h4 class="subtitle has-text-centered">
      Bike Repair
    </h4>
    <div class="hero-body">
      <div style="text-align:center;">
        <video id="teaser1" autoplay muted loop style="flex: 50%; max-width: 100%;">
          <source src="./static/images/bike.mov" type="video/mp4">
      </video>
      </div>

      <section class="section">
        <div class="container is-max-desktop">
          <!-- Method Section -->
          <div class="columns is-centered has-text-centered">
            <div class="column is-full-width">
              <hr>
              <h2 class="title is-3">EgoView Synthesis (Follow the Dance!)</h2>
              <br>
              <div style="display: flex; justify-content: center; align-items: center;">
                <!-- First video -->
                <video id="teaser1" autoplay muted loop style="width: 45%; margin-right: 10px;">
                  <source src="./static/images/output_video.mov" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
          
                <!-- Second video -->
                <video id="teaser2" autoplay muted loop style="width: 45%;">
                  <source src="./static/images/synthesis.mov" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
              
              <div class="content has-text-justified">
                <p>
                  With 4 camera set-up, we can enable egoview synthesis at anywhere in the scene, boosting possible embodied applications! [Left: gt provided by EgoExo4D, Right: our synthesised view] (Different colour is led by different camera sensors, foreground in our reconstruction has been removed)
                </p>
              </div>
            </div>
          </div>

          <section class="section">
            <div class="container is-max-desktop">
              <!-- Method Section -->
              <div class="columns is-centered has-text-centered">
                <div class="column is-full-width">
                  <hr>
                  <h2 class="title is-3">More Results!</h2>
                  <br>
      <h4 class="subtitle has-text-centered">
        HealthCare - CPR
      </h4>
      <div style="text-align:center;">
        <video id="teaser1" autoplay muted loop style="flex: 50%; max-width: 100%;">
          <source src="./static/images/csR2.mov" type="video/mp4">
      </video>
      </div>
      <h4 class="subtitle has-text-centered">
       Music - Piano
      </h4>
      <div style="text-align:center;">
        <video id="teaser1" autoplay muted loop style="flex: 50%; max-width: 100%;">
          <source src="./static/images/piano.mov" type="video/mp4">
      </video>
      </div>
      <h4 class="subtitle has-text-centered">
        Cooking - Scrumble Egg
       </h4>
      <div style="text-align:center;">
        <video id="teaser1" autoplay muted loop style="flex: 50%; max-width: 100%;">
          <source src="./static/images/cookk.mov" type="video/mp4">
      </video>
      </div>

      <h4 class="subtitle has-text-centered">
        Sports - Football
       </h4>

      <div style="text-align:center;">
        <video id="teaser1" autoplay muted loop style="flex: 50%; max-width: 100%;">
          <source src="./static/images/football.mov" type="video/mp4">
      </video>
      </div>
      <h4 class="subtitle has-text-centered">
        Panoptic - Baseball
       </h4>
      <div style="text-align:center;">
        <video id="teaser1" autoplay muted loop style="flex: 50%; max-width: 100%;">
          <source src="./static/images/panoptic.mov" type="video/mp4">
      </video>
      </div>




      <!-- <img class="rounded" src="./media/nice-slam/teaser.png" > -->
      <br>
    </div>
    <!--         <iframe width="800" height="400" src="https://www.youtube.com/embed/ycivCw1LmdE?si=zstMsf33Lgjytup5" title="Intro to PVT++" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
 -->
</section>

<section class="section">
  <div class="container is-max-desktop">
        <!-- Paper video. -->
        <!-- <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Video</h2>
            <div class="publication-video">
              <iframe src="https://www.youtube.com/embed/V5hYTz5os0M?rel=0&amp;showinfo=0"
                      frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </div>
          </div>
        </div> -->
        <!--/ Paper video. -->
      
    <!-- <br> -->
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We address the problem of <b>dynamic scene reconstruction from sparse-view videos.</b> Prior work often requires <b>dense multi-view captures using dozens of calibrated cameras</b> (e.g. Panoptic Studio), or <b>short monocular videos with limited information</b> (e.g. DAVIS). In contrast, we aim to reconstruct diverse dynamic human behaviors, such as repairing a bike or dancing from sparse-view videos. 
          </p>
          <p>
            We repurpose state-of-the-art monocular reconstruction methods for <b>sparse-view reconstruction</b> and find that <b>careful initialization from time- and view-consistent monocular depth estimators</b> produces more accurate reconstructions. 
            Specifically, our method predicts dense surface points across all training views, and uses confidence-aware pixel alignment to initialize scene geometry. 
            We further distill per-point semantic features from 2D foundation models, and use <b> feature clustering</b> to encode a compact set of motion bases. 
            Finally, we employ a gradient-based joint optimization framework to simultaneously learn scene geometry and motion. 
          </p>
          <p>
            Notably, our approach achieves <b>state-of-the-art performance</b> on challenging sequences from the Ego-Exo4D dataset.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- <br> -->
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Contribution</h2>
        <div class="content has-text-justified">
          <li>
            We repurpose Ego-Exo4D for sparse-view reconstruction and highlight the challenge of reconstructing skilled human behaviors in dynamic environments.
          <li>
            We demonstrate that monocular reconstruction methods can be extended to the sparse-view setting by carefully incorporating monocular depth and foundational priors.
          </li>
          <li>
            We extensively ablate our design choices and show that we achieve state-of-the-art performance on challenging sequences from Ego-Exo4D.
          </li>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method Section -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <hr>
        <h2 class="title is-3">Method Overview</h2>
        <br>
        <div class="content has-text-justified">
          <img src="static/images/method.png" alt="Structure" class="center" />
          <p>As dynamic scene reconstruction from sparse views is extremely challenging, we present two key insights to initialize plausible geometry and motion:</p>
              <li>Initializing consistent scene geometry via confidence-aware spatio-temporal alignment</li>
              <li>Initializing motion trajectories by clustering per-point 3D semantic features distilled from 2D foundation models</li>
            </ul>
          </ul>
        </div>
    

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method Section -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <hr>
        <h2 class="title is-3">Experiments</h2>
        <br>
        <div class="content has-text-justified">
          <h4>Concrete steps to validate our points:</h4>
          <ul>
              <li>
                  <strong>Perfect training views</strong> 
                  <span>(prove correct implementation)</span>
              </li>
              <li>
                  <strong>Perfect near-novel (5&deg;) view synthesis</strong> 
                  <span>(illustrate that the method works, like other approaches)</span>
              </li>
              <li>
                  <strong>Great extreme-novel (45&deg;) view synthesis</strong> 
                  <span>(demonstrate free-viewpoint rendering, no one did it before)</span>
              </li>
              <li>
                  <strong>Held-out camera evaluation (90&deg;)</strong> 
                  <span>(use 3 out of 4 cameras for training and leave 1 out for qualitative results)</span>
              </li>
          </ul>

          <div class="columns is-centered has-text-centered">
            <div class="column is-full-width">
              <h3>Selected Qualitative Results</h3>

          
              <div class="content has-text-justified">
                <h5><strong>45°</strong>  Novel View Synthesis Comparison (1)</h5>
                <img src="static/images/fig_main_qua.jpeg" alt="Structure" class="center" />
                <div class="content has-text-justified">
                  <h5><strong>45°</strong>  Novel View Synthesis Comparison (2)</h5>
                  <img src="static/images/fig_sub_qua.jpeg" alt="Structure" class="center" />
                  <div class="content has-text-justified">
                    <h5><strong>45°</strong>  Novel View Synthesis Comparison (3)</h5>
                    <img src="static/images/fig_append.jpeg" alt="Structure" class="center" />
              <p>
                  Existing monocular methods and their extension to multi-view produce poor results rendered from a drastically different novel view.
                  <strong>MV-SOM</strong> improves upon SOM in <strong>45°</strong> novel-view synthesis. 
                  Our method's careful point cloud initialization and feature-based motion bases further improve on MV-SOM. 
              </p>
                  </ul>
                </ul>





        </div>

<section class="section" >
  <div class="container is-max-desktop content">
    <h1 class="title">BibTeX</h1>
    yet to come, I will send you email to ask for citation (just kidding). 
  </div>
</section>


<section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h1 class="title">Acknowledgements</h1>
    Zihan would like to thank all the coauthor for discussion and paper writting. Beyond that, Jeff for deep discussion about this project, Tarasha for debugging advice, Neehar for high-level insight, and Prof. Deva Ramanan for insightful guidance and advice. Outside of the author list, we would like to thank Nikhil Keetha and Jay Karhade for their great suggestions.    
    We thank ourselves for surviving. We also extend our gratitude to Zihan for his generous payment of tuition fees to Carnegie Mellon University, which made this possible.
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            This webpage template is from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. 
            We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
          </p>
        </div>
      </div>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>