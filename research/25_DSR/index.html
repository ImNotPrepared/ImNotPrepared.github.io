<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"

        content="Why is Sparse-View 4D Reconstruction Hard?">
  <meta name="keywords" content="4D Dynamics Scene Reconstruction, Free-view Synthesis, Sparse View">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>4D Recon</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->
  <link rel="icon" type="image/png" href="./static/images/airlab.png"> 
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/airlab.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://imnotprepared.github.io/research/25_DSR">
            4D Scene Recon - in submission
          </a>
          <a class="navbar-item" href="https://imnotprepared.github.io/research/IROS24_AirShot">
            AirShot - IROS 2024
          </a>
          <a class="navbar-item" href="https://imnotprepared.github.io/research/ICLR24_ONLS/">
            ONLS - ICLR 2024
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="./static/images/tteaser.svg" width="300">
          <h1 class="title is-2 publication-title">Why is Sparse-View 4D Reconstruction Hard?</h1>
          <div class="column is-full_width">
            <h2 class="title is-3">in submission</h2>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://imnotprepared.github.io/">Zihan Wang</a>,</span>
            <span class="author-block">
              <a href="https://jefftan969.github.io/">Jeff Tan</a>,</span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~tkhurana/">Tarasha Khurana</a>,
            </span>
            <span class="author-block">
              <a href="https://www.neeharperi.com/">Neehar Peri</a>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan*</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Carnegie Mellon University
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/10XsArUhlpraBpuiVFpCH1TOwCm5C-dgy/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://drive.google.com/file/d/10XsArUhlpraBpuiVFpCH1TOwCm5C-dgy/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv(soon)</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code(soon)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data (ExoRecon, soon)</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div style="text-align:center;">
        <img src="static\images\teaser.png" style="width:100%; height:auto;">
      </div>
      <!-- <img class="rounded" src="./media/nice-slam/teaser.png" > -->
      <br>
      <h2 class="subtitle has-text-centered">
        Given <b>sparse-view videos of dynamic scenes</b>, our approach reconstructs 3D geometry and motion, enabling extreme novel view synthesis, 3D tracking, and feature distillation. <b>Our sparse-view (4-camera) setup strikes a balance</b> between ill-posed reconstructions from casual monocular captures and well-constrained reconstructions from dense multi-view studio captures.
      </h2>
    </div>

    <h2 class="subtitle has-text-centered">
      We show results in diverse scene such as bike repair, healthcare(CSR) and dancing! 
    </h2>
    <h1> 
      Complex, High occluded scene: Bike Repair
    </h1>
    <div class="hero-body">
      <div style="text-align:center;">
        <video id="teaser1" autoplay muted loop style="flex: 50%; max-width: 100%;">
          <source src="./static/images/bike.mov" type="video/mp4">
      </video>
      </div>

      <div style="text-align:center;">
        <video id="teaser1" autoplay muted loop style="flex: 50%; max-width: 100%;">
          <source src="./static/images/csr.mov" type="video/mp4">
      </video>
      </div>

      <div style="text-align:center;">
        <video id="teaser1" autoplay muted loop style="flex: 50%; max-width: 100%;">
          <source src="./static/images/dance.mov" type="video/mp4">
      </video>
      </div>
      <!-- <img class="rounded" src="./media/nice-slam/teaser.png" > -->
      <br>
      <h2 class="subtitle has-text-centered">
        <strong>RoboTools</strong> is a new benchmark for <b>Novel Instance Detection</b>.
      </h2>
    </div>
    <!--         <iframe width="800" height="400" src="https://www.youtube.com/embed/ycivCw1LmdE?si=zstMsf33Lgjytup5" title="Intro to PVT++" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
 -->
</section>

<section class="section">
  <div class="container is-max-desktop">
        <!-- Paper video. -->
        <!-- <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Video</h2>
            <div class="publication-video">
              <iframe src="https://www.youtube.com/embed/V5hYTz5os0M?rel=0&amp;showinfo=0"
                      frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </div>
          </div>
        </div> -->
        <!--/ Paper video. -->
      
    <!-- <br> -->
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We address the problem of <b>dynamic scene reconstruction from sparse-view videos.</b> Prior work often requires <b>dense multi-view captures using dozens of calibrated cameras</b> (e.g. Panoptic Studio), or <b>short monocular videos with limited information</b> (e.g. DAVIS). In contrast, we aim to reconstruct diverse dynamic human behaviors, such as repairing a bike or dancing from sparse-view videos. 
          </p>
          <p>
            We repurpose state-of-the-art monocular reconstruction methods for <b>sparse-view reconstruction</b> and find that <b>careful initialization from time- and view-consistent monocular depth estimators</b> produces more accurate reconstructions. 
            Specifically, our method predicts dense surface points across all training views, and uses confidence-aware pixel alignment to initialize scene geometry. 
            We further distill per-point semantic features from 2D foundation models, and use <b> feature clustering</b> to encode a compact set of motion bases. 
            Finally, we employ a gradient-based joint optimization framework to simultaneously learn scene geometry and motion. 
          </p>
          <p>
            Notably, our approach achieves <b>state-of-the-art performance</b> on challenging sequences from the Ego-Exo4D dataset.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- <br> -->
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Contribution</h2>
        <div class="content has-text-justified">
          <li>
            We propose the first 3D-geometry aware instance detector, VoxDet, which identifies any (novel) specifc instance in the wild.
          <li>
            We develop <b>Template Voxel Aggregation</b> and <b>Query Voxel Matching</b> mechanisms to represent and match instances, respectively.
          </li>
          <li>
            We discover that via <b>reconstruction pre-training</b>, the Voxel representation is much stronger and more generalizable.
          </li>
          <li>
            We compile the first novel instance detection benchmark <b>RoboTools</b> for evaluation and synthetic training set <b>OWID</b>, which are all publicly awailable.
          </li>
          <li>
            We conduct exhaustive experiments to validate the generalization capability and robustness of VoxDet against vairous traditional 2D models.
          </li>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full_width">
        <hr>
        <h2 class="title is-3">Method</h2>
        <br>
        <h3 class="title is-4">Model Structure</h3>
        <div class="content has-text-justified">
          <img src="static\images\Structure.png" class="center"/>
          <p>VoxDet consists of 3 main modules:
            <li><b>Open-World Detection Module</b>: Takes in an arbitary image and outputs open-world proposals that cover all potential objects. We obtain the 2D proposal feature (ROI) to be matched.</li>
            <li><b>Template Voxel Aggregation Module</b>: Consumes the multi-view reference images (and cooresponding camera extrinsic) as input, then construct a compact template voxel via the geometric relationship between each frame.</li>
            <li><b>Query Voxel Matching Module</b>: Matches the template voxel with each proposal by 2D-3D mapping and voxel repation. We find such matching is tailored for instances and robust to pose variations.</li>
          </p>
        </div>

        <h3 class="title is-4">Two Stage Training</h3>
        <div class="content has-text-justified">
          <p>
            <video id="teaser1" autoplay muted loop style="flex: 50%; max-width: 100%;">
              <source src="./static/images/two_stage.mp4" type="video/mp4">
          </video>
          <p>
          <p>We discover that two-stage training mechanism helps VoxDet learn the geometry of an instance and generalize better:
            <li><b>Stage 1 Reconstruction</b>: The 2D-3D mapping needs to construct 3D voxel from 2D image features, we discover that this module needs to be pre-trained via reconstruction objective.</li>
            <li><b>Stage 2 Detection</b>: We first initialize the 2D-3D mapping blocks in TVA and QVM, then we used a smaller learning rate to learn the voxel representation tailored for detection task.</li>
            <li><b>(Optional) Stage 3 Rotation estimation</b>: The rotation measurement in QVM can have additional supervision, which slightly improves performance and is optional.</li>
          </p>
        </div>
      </div>
    </div>
    <hr>

    
    <h3 class="title is-4">Comparison with Gen6D</h3>
    <video id="teaser1" autoplay muted loop style="flex: 50%; max-width: 100%;">
        <source src="./static/images/Comp_Gen6d.mp4" type="video/mp4">
    </video>
    <h2 class="subtitle has-text-centered">
      Gen6D falls short when the instance is in unseen poses or occluded.
    </h2>

    <p>
      &nbsp
    </p>
    <h3 class="title is-4">Comparison with DTOID</h3>
    <video id="teaser1" autoplay muted loop style="flex: 50%; max-width: 100%;">
        <source src="./static/images/Comp_DTOID.mp4" type="video/mp4">
    </video>
    <h2 class="subtitle has-text-centered">
      DTOID can't handel complex backgrounds very well.
    </h2>

    <p>
      &nbsp
    </p>
    <h3 class="title is-4">Comparison with OLN_DINO</h3>
    <video id="teaser1" autoplay muted loop style="flex: 50%; max-width: 100%;">
        <source src="./static/images/Comp_OLN.mp4" type="video/mp4">
    </video>
    </div>
    <h2 class="subtitle has-text-centered">
      OLN_DINO has trouble distinguishing instances that have similar semantics.
    </h2>
  </div>
</section>



<section class="section" >
  <div class="container is-max-desktop content">
    <h1 class="title">BibTeX</h1>
    yet to come, I will send you email to ask for citation (just kidding). 
  </div>
</section>


<section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h1 class="title">Acknowledgements</h1>
    We thank ourselves for surviving. We also extend our gratitude to Zihan for his generous payment of tuition fees to Carnegie Mellon University, which made this possible.
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            This webpage template is from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. 
            We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
          </p>
        </div>
      </div>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>