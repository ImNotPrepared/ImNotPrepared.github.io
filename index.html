<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Zihan Wang</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Zihan Wang">
  <meta name="description" content="Zihan Wang - Robotics Institute, Carnegie Mellon University">
  <link rel="icon" type="image/png" href="/img/headers/RI.png">
  <link rel="stylesheet" type="text/css" href="./files/stylesheet.css">
  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?c83bf8b7e066e7c41af36a85a5651bf2";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Zihan Wang</name>
                  </p>
                  <p style="text-align:center">
                    Robotics Institute, Carnegie Mellon University
                  </p>
                  <p>
                    <i>"TLDR: Talk is Cheap, just show me <a href="#selected-publications">Publications?</a>"</i>
                  </p>
                  <p>
                    I'm a Master's student in Computer Vision (<b>MSCV</b>) at the <a href="https://www.ri.cmu.edu/">Robotics Institute</a>, <a href="https://www.cs.cmu.edu/">School of Computer Science</a>, <a href="https://www.cmu.edu/">Carnegie Mellon University</a>, advised by Prof. <a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan</a>. My research focuses on 4D reconstruction and its applications to robotics.
                  </p>
                  <p>
                    Prior to CMU, I obtained my B.Eng. with Honours in Electrical and Electronic Engineering from the <a href="https://www.ed.ac.uk/">University of Edinburgh</a>, where I worked on counterfactual estimation with the <a href="https://www.chai.ac.uk/">Causality in Healthcare AI Hub</a>. During my undergraduate years, I also spent time at <a href="https://theairlab.org/">AIRLab</a> working with Prof. <a href="https://theairlab.org/team/sebastian/">Sebastian Scherer</a> and Prof. <a href="https://sairlab.org/team/chen/">Chen Wang</a>, and later visited <a href="https://bair.berkeley.edu/">BAIR</a> at UC Berkeley focusing on reinforcement learning.
                  </p>
                  <p>
                    I build machine learning, computer vision, and robotics systems that close the loop between perception and action. If you're exploring robotics projects that need strong vision capabilities, feel free to <a href="mailto:zihanwa3@cs.cmu.edu">drop me a line</a>—I'm always open to collaboration.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:zihanwa3@cs.cmu.edu">Email</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?hl=en&user=uWf4E0YAAAAJ" target="_blank">Google Scholar</a> &nbsp;/&nbsp;
                    <a href="https://github.com/Z1hanW" target="_blank">GitHub</a> &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/zihan-w-06a684225/" target="_blank">LinkedIn</a> &nbsp;/&nbsp;
                    <a href="https://x.com/1mNotPrepared" target="_blank">X</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:37%;max-width:37%;vertical-align:middle">
                  <a href="/img/zihan.jpg">
                    <img style="width:100%;max-width:100%;border-radius:8px;" alt="Zihan Wang" src="/img/zihan.jpg">
                  </a>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    We live in a dynamic 4D world, constantly seeing, understanding, and interacting with objects and environments. My goal is to bridge the gap between reconstructed virtual worlds and practical robotic applications. I develop methods that help robots perceive, understand, and interact with objects and environments from sparse observations.
                  </p>
                  <p>
                    I mainly work across machine learning, computer vision, and robotics. The specific research directions evolve with projects—browse the highlights below or reach out if you'd like to collaborate.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading id="selected-publications">Selected Publications</heading>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:30%;vertical-align:middle">
                  <video autoplay muted loop playsinline style="width:100%;border-radius:6px;">
                    <source src="/img/paper_teaser/crispdemo.mp4" type="video/mp4">
                  </video>
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>CRISP: Contact-guided Real2Sim from Monocular Video with Planar Scene Primitives</papertitle>
                  <br>
                  <span class="author-me">Zihan Wang</span>,
                  <a class="author-other" href="https://jiashunwang.github.io/" target="_blank">Jiashun Wang</a>,
                  <a class="author-other" href="https://jefftan969.github.io/" target="_blank">Jeff Tan</a>,
                  <a class="author-other" href="https://tsukasane.github.io/" target="_blank">Yiwen Zhao</a>,
                  <a class="author-other" href="https://www.cs.cmu.edu/~jkh/" target="_blank">Jessica Hodgins</a>,
                  <a class="author-other" href="https://shubhtuls.github.io/" target="_blank">Shubham Tulsiani</a>,
                  <a class="author-other" href="https://www.cs.cmu.edu/~deva/" target="_blank">Deva Ramanan*</a>
                  <br>
                  In submission
                  <br>
                  <span>paper (coming soon)</span> /
                  <a href="https://Z1hanW.github.io/research/25_CRISP/" target="_blank">project page</a> /
                  <span>code &amp; data (coming soon)</span>
                  <p>
                    <i>A real-to-sim framework that turns monocular RGB video into whole-body contact-rich control across diverse, cluttered terrains.</i>
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:10px;width:30%;vertical-align:middle">
                  <video autoplay muted loop playsinline style="width:100%;border-radius:6px;">
                    <source src="/img/paper_teaser/demo.mp4" type="video/mp4">
                  </video>
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>MonoFusion: Sparse-View 4D Reconstruction via Monocular Fusion</papertitle>
                  <br>
                  <span class="author-me">Zihan Wang</span>,
                  <a class="author-other" href="https://jefftan969.github.io/" target="_blank">Jeff Tan</a>,
                  <a class="author-other" href="https://www.cs.cmu.edu/~tkhurana/" target="_blank">Tarasha Khurana</a>,
                  <a class="author-other" href="https://www.neeharperi.com/" target="_blank">Neehar Peri</a>,
                  <a class="author-other" href="https://www.cs.cmu.edu/~deva/" target="_blank">Deva Ramanan*</a>
                  <br>
                  International Conference on Computer Vision (<b>ICCV</b>), 2025
                  <br>
                  <a href="https://drive.google.com/file/d/10XsArUhlpraBpuiVFpCH1TOwCm5C-dgy/view?usp=sharing" target="_blank">paper</a> /
                  <a href="https://Z1hanW.github.io/research/25_DSR/" target="_blank">project page</a> /
                  <a href="https://github.com/Z1hanW/MonoFusion" target="_blank">code</a>
                  <p>
                    <i>The first approach that enables free-view synthesis for 4D dynamic scene reconstruction under sparse-view capture.</i>
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:10px;width:30%;vertical-align:middle">
                  <video autoplay muted loop playsinline style="width:100%;border-radius:6px;">
                    <source src="/img/headers/airdet.mp4" type="video/mp4">
                  </video>
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>AirShot: Efficient Few-Shot Detection for Autonomous Exploration</papertitle>
                  <br>
                  <span class="author-me">Zihan Wang</span>,
                  <a class="author-other" href="https://scholar.google.com/citations?user=XjZjcakAAAAJ&hl=en" target="_blank">Bowen Li</a>,
                  <a class="author-other" href="https://scholar.google.com/citations?user=vZfmKl4AAAAJ&hl=en" target="_blank">Chen Wang</a>,
                  <a class="author-other" href="https://scholar.google.com/citations?user=gxoPfIYAAAAJ&hl=en" target="_blank">Sebastian Scherer*</a>
                  <br>
                  IEEE/RSJ International Conference on Intelligent Robots and Systems (<b>IROS</b>), 2024
                  <br>
                  <a href="https://arxiv.org/abs/2404.05069" target="_blank">paper</a> /
                  <a href="https://Z1hanW.github.io/research/IROS24_AirShot" target="_blank">project page</a> /
                  <a href="https://github.com/Z1hanW/AirShot" target="_blank">code</a>
                  <p>
                    <i>Class-agnostic relations for few-shot detection without fine-tuning, enabling fast and efficient field deployment.</i>
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:10px;width:30%;vertical-align:middle">
                  <img src="/img/paper_teaser/NML.jpg" alt="Learning with Noisy Foundation Models" style="width:100%;border-radius:6px;">
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>Learning with Noisy Foundation Models</papertitle>
                  <br>
                  <span class="author-other">Hao Chen</span>,
                  <span class="author-me">Zihan Wang</span>,
                  <span class="author-other">Jindong Wang</span>,
                  <span class="author-other">Ran Tao</span>,
                  <span class="author-other">Hongxin Wei</span>,
                  <span class="author-other">Xing Xie</span>,
                  <span class="author-other">Masashi Sugiyama</span>,
                  <span class="author-other">Bhiksha Raj</span>
                  <br>
                  IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2024
                  <br>
                  <a href="https://arxiv.org/abs/2403.06869" target="_blank">paper</a> /
                  <a href="https://arxiv.org/abs/2403.06869" target="_blank">project page</a>
                  <p>
                    <i>Characterizes noise in large-scale pre-training data and proposes strategies to mitigate its impact on downstream tasks.</i>
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:10px;width:30%;vertical-align:middle">
                  <video autoplay muted loop playsinline style="width:100%;border-radius:6px;">
                    <source src="/img/paper_teaser/hdmi.mp4" type="video/mp4">
                  </video>
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>HDMI: Learning Interactive Humanoid Whole-Body Control from Human Videos</papertitle>
                  <br>
                  <a class="author-other" href="https://egalahad.github.io/" target="_blank">Haoyang Weng</a>,
                  <a class="author-other" href="https://liyitang22.github.io/" target="_blank">Yitang Li</a>,
                  <a class="author-other" href="https://github.com/Nike353/" target="_blank">Nikhil Sobanbabu</a>,
                  <span class="author-me">Zihan Wang</span>,
                  <a class="author-other" href="https://www.zhengyiluo.com/" target="_blank">Zhengyi Luo</a>,
                  <a class="author-other" href="https://tairanhe.com/" target="_blank">Tairan He</a>,
                  <a class="author-other" href="https://www.cs.cmu.edu/~deva/" target="_blank">Deva Ramanan</a>,
                  <a class="author-other" href="https://www.gshi.me/" target="_blank">Guanya Shi*</a>
                  <br>
                  In submission
                  <br>
                  <a href="https://arxiv.org/abs/2509.16757" target="_blank">paper</a> /
                  <a href="https://hdmi-humanoid.github.io/#/" target="_blank">project page</a> /
                  <a href="https://github.com/LeCAR-Lab/HDMI" target="_blank">code</a>
                  <p>
                    <i>A general framework that learns whole-body humanoid-object interaction skills directly from monocular RGB videos.</i>
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:10px;width:30%;vertical-align:middle">
                  <img src="/img/paper_teaser/CF.jpg" alt="Identity Preservation for Counterfactual Estimation" style="width:100%;border-radius:6px;">
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>Identity Preservation for Counterfactual Estimation</papertitle>
                  <br>
                  <span class="author-me">Zihan Wang</span>
                  <br>
                  Bachelor Thesis, 2023
                  <p>
                    <i>Cross-sectional counterfactual estimation conditioned on disentangled identity-preserving features.</i>
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:10px;width:30%;vertical-align:middle">
                  <img src="/img/paper_teaser/ONLS.jpg" alt="ONLS" style="width:100%;border-radius:6px;">
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>ONLS: Optimal Noise Level Search in Diffusion Autoencoders Without Fine-Tuning</papertitle>
                  <br>
                  <span class="author-me">Zihan Wang</span>
                  <br>
                  International Conference on Learning Representations (<b>ICLR</b>) Workshop, 2024
                  <p>
                    <i>A simple, effective, fine-tuning-free strategy to pick the optimal diffusion depth for each sample.</i>
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:10px;width:30%;vertical-align:middle">
                  <img src="/img/paper_teaser/pypose.jpg" alt="PyPose" style="width:100%;border-radius:6px;">
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>PyPose: A Library for Robot Learning with Physics-based Optimization</papertitle>
                  <br>
                  <span class="author-other">PyPose Team</span>
                  <br>
                  IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022 &amp; IEEE/RSJ IROS Workshop, 2023
                  <br>
                  <a href="https://arxiv.org/abs/2209.15428" target="_blank">paper</a> /
                  <a href="https://arxiv.org/abs/2309.13035" target="_blank">extension</a> /
                  <a href="https://pypose.org/" target="_blank">project page</a> /
                  <a href="https://github.com/pypose/pypose" target="_blank">code</a> /
                  <a href="https://youtu.be/XDtUDIWuGng?si=UC3tkuuEVIrbma6E" target="_blank">video</a>
                  <p>
                    <i>Bridges classic robotics and modern learning with a physics-aware optimization toolkit.</i>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Projects</heading>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:30%;vertical-align:middle">
                  <video autoplay muted loop playsinline style="width:100%;border-radius:6px;">
                    <source src="/img/project_teaser/diverse.mp4" type="video/mp4">
                  </video>
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>A Style-Preserved Motion Controller for Simulated Humanoid Characters</papertitle>
                  <br>
                  Robot Learning
                  <br>
                  <a href="https://arxiv.org/abs/2209.15428" target="_blank">report</a>
                  <p>
                    <i>Clusters motion data, trains latent variable models, and deploys a style-aware high-level controller for multi-task behaviors.</i>
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:10px;width:30%;vertical-align:middle">
                  <img src="/img/paper_teaser/pypose.jpg" alt="PyPose" style="width:100%;border-radius:6px;">
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>PyPose: Physics-based Optimization for Robot Learning</papertitle>
                  <br>
                  <a href="https://arxiv.org/abs/2209.15428" target="_blank">paper</a> /
                  <a href="https://arxiv.org/abs/2309.13035" target="_blank">extension</a> /
                  <a href="https://pypose.org/" target="_blank">project page</a> /
                  <a href="https://github.com/pypose/pypose" target="_blank">code</a>
                  <p>
                    <i>Contributed physics-aware constrained optimization modules for trajectory planning and motion control.</i>
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:10px;width:30%;vertical-align:middle">
                  <img src="/img/project_teaser/monai.png" alt="MONAI" style="width:100%;border-radius:6px;">
                </td>
                <td style="padding:20px;width:70%;vertical-align:middle">
                  <papertitle>MONAI: Medical Open Network for Artificial Intelligence</papertitle>
                  <br>
                  <a href="https://monai.io/" target="_blank">project page</a> /
                  <a href="https://github.com/Project-MONAI" target="_blank">code</a> /
                  <a href="https://www.youtube.com/watch?v=j7nebK8Il_g&t=14s" target="_blank">video</a>
                  <p>
                    <i>Developed diffusion autoencoder components for medical imaging, with a focus on MRI generation and reconstruction.</i>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Awards</heading>
                  <ul>
                    <li>Edinburgh Award (Research Experience), 2022</li>
                    <li>Edinburgh Scholarship (£4,000), 2022</li>
                    <li>Edinburgh Scholarship (£4,000), 2021</li>
                    <li>QIANJIANG ELECTRIC Scholarship (£3,000), 2020</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Service</heading>
                  <p><strong>Academic Reviewer</strong></p>
                  <ul>
                    <li>International Conference on Learning Representations (ICLR)</li>
                    <li>European Conference on Computer Vision (ECCV)</li>
                    <li>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</li>
                    <li>IEEE Transactions on Image Processing (TIP)</li>
                    <li>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Advisors &amp; Mentors</heading>
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-top:10px;">
                    <tbody>
                      <tr>
                        <td style="padding:15px;text-align:center;vertical-align:top;width:25%;">
                          <a href="https://www.cs.cmu.edu/~deva/" target="_blank">
                            <img src="/img/best_prof_cv_god.svg" alt="Deva Ramanan" style="height:140px;">
                          </a>
                          <div>Prof. Deva Ramanan @ CMU RI</div>
                        </td>
                        <td style="padding:15px;text-align:center;vertical-align:top;width:25%;">
                          <a href="https://scholar.google.com/citations?user=gxoPfIYAAAAJ&hl=de" target="_blank">
                            <img src="/img/basti.svg" alt="Sebastian Scherer" style="height:140px;">
                          </a>
                          <div>Prof. Sebastian Scherer @ CMU RI</div>
                        </td>
                        <td style="padding:15px;text-align:center;vertical-align:top;width:25%;">
                          <a href="https://scholar.google.com/citations?user=vZfmKl4AAAAJ&hl=en" target="_blank">
                            <img src="/img/chen.svg" alt="Chen Wang" style="height:140px;">
                          </a>
                          <div>Prof. Chen Wang @ University at Buffalo</div>
                        </td>
                        <td style="padding:15px;text-align:center;vertical-align:top;width:25%;">
                          <a href="https://scholar.google.com/citations?user=jC1uFnYAAAAJ&hl=en" target="_blank">
                            <img src="/img/sotos.svg" alt="Sotirios Tsaftaris" style="height:140px;">
                          </a>
                          <div>Prof. Sotirios Tsaftaris @ University of Edinburgh</div>
                        </td>
                      </tr>
                    </tbody>
                  </table>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Contact</heading>
                  <ul>
                    <li>Email: <a href="mailto:zihanwa3@cs.cmu.edu">zihanwa3@cs.cmu.edu</a></li>
                    <li>Phone: +1-412-589-8046</li>
                    <li>Address: EDSH, Carnegie Mellon University, Pittsburgh, PA 15213, USA</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Visitors</heading>
                  <p>
                    I am confident that my "boundaryless", "website-stalking" friends have contributed to at least 74.47% of the traffic here.
                  </p>
                  <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=Ut5Cyesc_7HlQfBOsRrD25kuA___MbMlwP3C2FdZj-I&cl=ffffff&w=a"></script>
                  <div style="margin-top:20px;text-align:center;">
                    <script type="text/javascript" src="//rf.revolvermaps.com/0/0/7.js?i=5r6hi9cynss&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;sx=0" async="async"></script>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    Template adapted from <a href="https://qitaozhao.github.io" target="_blank">Qitao Zhao</a> (credit to <a href="https://yijiaweng.github.io" target="_blank">Yijia Weng</a> &amp; <a href="https://jonbarron.info/" target="_blank">Jon Barron</a>).<br>
                    Last updated: February 2025
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
    </tbody>
  </table>
</body>
</html>
